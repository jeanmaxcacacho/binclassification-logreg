---
title: "Supervised Classification of Diabetes Data"
author: Jean Maximus C. Cacacho
format: html
---

# Introduction

The data used to perform this *study* was obtained from an [open-access Kaggle data repository](https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset). The link to the data repository is:

https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset

The primary objective of this study is to perform a **binary classification task with logistic regression**, identifying diabetic from non-diabetic observations. Second to this would be examining the effect of two different resampling approaches: **(i) oversampling with Random Oversampling** and **(ii) undersampling with Random Undersampling**, to the performance of the model.

# Data Pre-processing

To yield a performant model, it's imperative for our input data to be clean. This involves pruning observations from the original data file, scaling values, and the appropriate value conversions, among other appropriate steps that may arise.

On a high level, what is done in this subsection are the following:

1. Loading the dataset into R studio.
2. Null checks.
3. Encoding.

## Loading Data
In this step the `diabetes_prediction_dataset.csv` file is stored into a table.
```{r}
library(data.table)
raw_df <- fread("../data/diabetes_prediction_dataset.csv")
```
After this, the head and tail of the loaded dataframe was inspected to see if the file was read into the environment.

### Head
```{r}
head(raw_df)
```
### Tail
```{r}
tail(raw_df)
```

## Removing Nulls
Before further pre-processing steps (scaling, value conversion etc.), observations that aren't complete will be removed. Essentially rows with missing data are to be discarded from the considered dataset.

### Counting Nulls / Attribute
```{r}
colnames(raw_df)
```
From this cell it can be gathered that there are 9 attributes all in all. From the whole dataset with respect to each of these attributes, we count the amount of nulls.
```{r}
null_counts <- raw_df[, lapply(.SD, function(x) sum(is.na(x) | trimws(x) == ""))]
null_counts
```
The result of the previous cell shows that all rows have complete data.

## Type Conversions
Part of data pre-processing is also ensuring that observations are stored in programmatically convenient formats. [In the context of R an example of this would be storing categorical data as factors](https://www.youtube.com/watch?v=C4N3_XJJ-jU).

`str(raw_df)` can be used to inspect how these values are stored in the table.
```{r}
str(raw_df)
```
The previous query showed that the following attributes: (i) gender, (ii) hypertension, (iii) heart_disease, (iv) smoking_history, and (v) diabetes, are categorical. However, the attribute smoking_history seems to have more than just 2 possible values.

To be sure, an inspection of all unique values in these attributes ascertains whether or not the value is binary or not.
```{r}
categorical_attr <- c("gender", "hypertension", "heart_disease", "smoking_history", "diabetes")

unique_vals <- lapply(raw_df[, ..categorical_attr], unique)
print(unique_vals)
```
It can be seen that the attributes: hypertension, heart_disease, and diabetes, already only have two possible values; gender and smoking_history however do not.

### Handling `gender` Attribute
Surface level research shows that individuals with non-binary gender identities show higher incidence rates for other diabetes related comorbidities, e.g. smoking [(Tan et. al. 2021)](https://pmc.ncbi.nlm.nih.gov/articles/PMC8529476/#:~:text=Transgender%20and%20gender%2Dexpansive%20(TGE)%20adults%E2%80%94individuals%20who%20have,to%20smoke%20cigarettes%20than%20cisgender%20individuals%20[3].). This suggests that gender identities beyond just male and female may provide the model with sufficient information, along with other features, for classification.

| gender | description                          | encoding |
|------------------|--------------------------------------|----------:|
| Male            | Individual whose sex is Male                  |         0 |
| Female           | Individual whose sex is Female                  |         1 |
| Other      | Individual with non-binary gender identity       |         2 |

These encodings serve only to be labels that are programmatically convenient to work with in processing.
```{r}
gender_encodings = c(
  "Male" = 0,
  "Female" = 1,
  "Other" = 2
)

raw_df[, gender_code := gender_encodings[gender]]
print(raw_df)
```
It can be seen that `gender` has been successfully encoded into `gender_code`. As such, the original column can now be discarded.
```{r}
raw_df[, gender := NULL]
print(raw_df)
```
### Handling `smoking_history` Attribute
The earlier cell that was supposed to detect and count for null values was not able to catch `smoking_history` observations marked with `No Info`. That was because the previous function looked for values stored in R as `NA` or as empty strings. Including these observations may just be *noise* for the classifier, as such observations with `No Info` in `smoking_history` are discarded.
```{r}
row_count_before <- raw_df[, .N] # row count before drop
print(paste("Row count before drop: ", row_count_before))

raw_df <- raw_df[!(smoking_history == "No Info")]

unique(raw_df$smoking_history)
row_count_after <- raw_df[, .N] # row count after drop
print(paste("Row count after drop: ", row_count_after))
```
To turn the remaining string values into meaningful numerical data, **ordinal encoding comes to mind.** Notice how the magnitude of `smoking_history` can be surmised from the current values; those who have never smoked have, by definition, smoked less than those who were former smokers. Through the same intuition, it can be said that current smokers smoke the most. Formal definitions for these terms can be obtained from [online medical sources](https://www.cdc.gov/mmwr/volumes/72/wr/mm7210a7.htm).

From this information these encodings were generated:

| smoking_history | description                          | encoding |
|------------------|--------------------------------------|----------:|
| never            | never smoked before, or has smoked less than 100 cigarettes in their lifetime                  |         0 |
| former           | smoked at least 100 cigarettes in their lifetime but does not currently smoke                  |         1 |
| not current      | smoked at least 100 cigarettes in their lifetime but does not currently smoke       |         1 |
| ever             | term encompassing both current and former smokers, anyone who has smoked at least 100 cigarettes in their lifetime     |         1 |
| current          | smoked at least 100 cigarettes in their lifetime and currently smokes                        |         2 |

It must be noted that in the context of this project, the terms `former`, `ever`, and `not current`, were not defined in the data card. As such, operative definitions were gleaned from the CDC *QuickStats* article hyperlinked above.
```{r}
smoking_encodings <- c(
  "never" = 0,
  "former" = 1,
  "not current" = 1,
  "ever" = 1,
  "current" = 2
)

raw_df[, smoking_code := smoking_encodings[smoking_history]]

print(raw_df)
```
It can be seen that `smoking_history` has been encoded successfully into the column smoking_code; the old column can now be safely discarded.
```{r}
raw_df[, smoking_history := NULL]

print(raw_df)
```
Now that the data is clean, categorical columns can now be converted to the `factor` data type.
```{r}
categorical_attr2 <- c("gender_code", "hypertension", "heart_disease", "smoking_code", "diabetes")

raw_df[, (categorical_attr2) := lapply(.SD, as.factor), .SDcols = categorical_attr2]

# verify changes
str(raw_df)
```

# Exploratory Data Analysis

The goal in exploratory data analysis is discovering correlations and distributions across all attributes. This is important as in this project, it serves as the main guide for feature selection.

Excluding the target, the current cleaned table has 8 attributes of which 4 are categorical (`hypertension`, `heart_disease`, `gender_code`, and `smoking_code`) and the other 4 (`age`, `bmi`, `HbA1c_level`, `blood_glucose_level`) are continuous.

## Analyzing Target Distribution

One of the biggest hurdles in classification tasks is class imbalance in the data; when there are more observations for a certain dependent variable than others then there is a class imbalance. In the context of the project the dependent variable is the `diabetes` attribute.

```{r}
counts <- table(raw_df$diabetes)

barplot(
  counts,
  names.arg = c("Non-diabetic", "Diabetic"),
  col = "steelblue",
  main = "Target Variable Distribution",
  xlab = "Classification",
  ylab = "Observation Frequency")
```

Visually it can be seen that the data is **greatly imbalanced**.

## Examination of Continuous Attributes

For this subsection we consider the continuous attributes of the dataset and how they relate to the target variable.
```{r}
continuous_attr <- c("age", "bmi", "HbA1c_level", "blood_glucose_level")
continuous_attr_histdata <- raw_df[, ..continuous_attr]

par(mfrow = c(2, 2))
for (attr in continuous_attr) {
  hist(continuous_attr_histdata[[attr]],
       main = paste("Histogram of", attr),
       col = "lightblue",
       xlab = attr)
}
```

The correlations of these continuous data attributes is shown in the correlation matrix below:
```{r}
cor_matrix <- cor(continuous_attr_histdata, use = "complete.obs")

print(cor_matrix)
```

Correlation between these variables seem very weak. No strong linear relationship can be made with any of them with each other. Given that EDA serves as the primary guide of feature selection in the project, it can be inferred that these attributes are not linearly redundant. This justifies the inclusion of all of these attributes into the feature set.

## Examination of Categorical Attributes

For this subsection we consider the categorical attributes of the dataset and how they relate to the target variable.
```{r}
categorical_attr3 <- c("hypertension", "heart_disease", "gender_code", "smoking_code")
categorical_attr_notarg <- raw_df[, ..categorical_attr3]

par(mfrow = c(2,2))
for (attr in categorical_attr3) {
  barplot(table(categorical_attr_notarg[[attr]]),
          main = paste("Distribution of", attr),
          col = "lightgreen")
}
```
For an analysis of these attributes the chi-square test for independence can be used to see *correlation* between the attribute and target.
```{r}
categorical_attr_wtarg <- c(categorical_attr3, "diabetes")
categorical_eda <- raw_df[, ..categorical_attr_wtarg]

# Chi-square test: Hypertension v.s. Diabetes
chisq.test(table(categorical_eda$hypertension, categorical_eda$diabetes))

# Chi-square test: Heart Disease v.s. Diabetes
chisq.test(table(categorical_eda$heart_disease, categorical_eda$diabetes))

# Chi-square test: Gender v.s. Diabetes
chisq.test(table(categorical_eda$gender_code, categorical_eda$diabetes))

# Chi-square test: Smoking v.s. Diabetes
chisq.test(table(categorical_eda$smoking_code, categorical_eda$diabetes))
```
All p-values obtained are very small (p < 2.2e-16) which provides strong evidence to reject the null-hypothesis. For the attributes `hypertension`, `heart_disease`, and `smoking_code` this is to be expected since they are known comorbidities for many diseases. However, `gender_code` also yielding low p-values suggests that gender (not sex, since non-binary identites are included) is statistically significant when analyzing diabetes distribution.

All that being said, failure to reject the null hypothesis for any of the variables suggests that they should be included to the final feature set.

# Modeling

From EDA, through the use of correlation matrix and Chi-square tests, the final feature set was obtained. Continuous variables were not colinear with each other and all categorical variables yielded small p-values, as such all nine features from the base dataset is used in modeling.

In this section logistic regression is run three times: (i) without resampling, (ii) resampling with ROS, and (iii) resampling with RUS. The target data distribution is heavily imbalanced, making the dataset a good candidate for analyzing performance contributions/effects of resampling techniques.

## Scaling Continuous Attributes

An inspection of the data's continuous attributes (e.g. `age`, `bmi`, `blood_glucose_level`) makes apparent the difference in *scale* that these values have. From a numerical standpoint, measurements in `bmi` and `age` are significantly less than measurements in `blood_glucose_level`.

Feature scaling ensures that all features contribute equally to the model's learning process. Numerically large features, when not scaled properly, may dominate learning over smaller-scale features; `blood_glucose_level` may, by virtue of the units it's measured in, overtake `HbA1c_level` in contribution because of the difference in their values.

`scale` performs z-score scaling, centering values around 0.
```{r}
raw_df$age <- scale(raw_df$age)
raw_df$bmi <- scale(raw_df$bmi)
raw_df$HbA1c_level <- scale(raw_df$HbA1c_level)
raw_df$blood_glucose_level <- scale(raw_df$blood_glucose_level)

head(raw_df[, ..continuous_attr])
```
## Logistic Regression without Resampling

In this subsection, logistic regression is performed and evaluated for accuracy without resampling.
```{r}
for_baseline <- copy(raw_df)
```
### Train & Test Set Creation
```{r}
set.seed("123")

# there are 60k rows so let's try 80-20
split_baseline <- caTools::sample.split(for_baseline$diabetes, SplitRatio = 0.8)

baseline_train_data <- subset(for_baseline, split_baseline == TRUE)
baseline_test_data <- subset(for_baseline, split_baseline == FALSE)
```
### Model Training
```{r}
baseline_model <- glm(diabetes ~ .,
                      data = baseline_train_data,
                      family = binomial)
summary(baseline_model)
```
### Making Predictions on `baseline_test_data`
```{r}
baseline_test_probs <- predict(baseline_model,
                               newdata = baseline_test_data,
                               type = "response")
baseline_test_pred <- ifelse(baseline_test_probs > 0.5, 1, 0)

# report accuracy
mean(baseline_test_pred == baseline_test_data$diabetes)
```
Without any resampling, the probability of an accurate prediction is 94.9%? That's higher than most people's final grade in CSCI 21! Recall that the target distribution is greatly imbalanced, this is may be a case of the *accuracy paradox* where the model correctly makes predictions on the majority class and neglects the minority class.

### Confusion Matrix
```{r}
baseline_confmatrix <- caret::confusionMatrix(
  as.factor(baseline_test_data$diabetes),
  as.factor(baseline_test_pred)
)

print(baseline_confmatrix)
```
The confusion matrix sheds more light on the model's performance. For cases where the model is tested on diabetic data (the correct prediction is 1), the model does not perform so well. In the second line of the confusion matrix the ratio between false positives and true positives is 500:909.

PPV and NPV values make more apparent the *accuracy paradox*. The PPV being 98.7% means that when the model makes a 0 prediction (non-diabetic), it is correct 98.7% of the time. In contrast, the NPV being 64.5% means that when the model predicts a 1 prediction (diabetic), it is only correct 64.5% of the time. **Given the class imbalance, the model will still get a high accuracy mark if it just predicted all observations to be non-diabetic.**

## Logistic Regression with Resampling

The previous subsection showed room for improvement with the NPV of the model. One of the main challenges in classification tasks is deciding how to handle class imbalances, one of the ways this is addressed is through resampling. Resampling, makes it so that the proportions in the training data is more balanced, giving the model more exposure to both targets.

The resampling approaches used in this project are: **(i) Oversampling with ROS, and (ii) Undersampling with RUS**. 

### Train & Test Set Creation for ROS
```{r}
for_ROS <- copy(raw_df)

set.seed("123")

split_ROS <- caTools::sample.split(for_ROS$diabetes, SplitRatio = 0.8)

ROS_train_data <- subset(for_ROS, split_ROS == TRUE)
ROS_test_data <- subset(for_ROS, split_ROS == FALSE)
```
### ROS Resampling

ROS works by randomly duplicating rows of the minority class, adding these dupes up until both classes have roughly the same number of instances. As such, plotting the training data before and after resampling shows the effect of ROS on the observed class imbalance.

```{r}
ROS_targcounts <- table(ROS_train_data$diabetes)

barplot(
  ROS_targcounts,
  names.arg = c("Non-diabetic", "Diabetic"),
  col = "steelblue",
  main = "Target Variable Distribution Before ROS",
  xlab = "Classification",
  ylab = "Observation Frequency")
```

The distribution in the bar graph mirrors the one shown in EDA previously.
```{r}
ROS_targcounts
```

Getting an approximate ratio...
```{r}
targ_ratio <- 45710 / 5637
targ_ratio
```
The majority class outnumbers the minority class by about 8 to 1. 
```{r}
ROS_output <- ROSE::ovun.sample(
  diabetes ~ .,
  data = ROS_train_data,
  method = "over",
  N = 2*max(table(ROS_train_data$diabetes))
)

ROS_train_data <- ROS_output$data
```

The new target distribution is...
```{r}
ROS_targcounts2 <- table(ROS_train_data$diabetes)

barplot(
  ROS_targcounts2,
  names.arg = c("Non-diabetic", "Diabetic"),
  col = "steelblue",
  main = "Target Variable Distribution After ROS",
  xlab = "Classification",
  ylab = "Observation Frequency")
```
### Model Training on ROS Data
```{r}
ROS_model <- glm(diabetes ~ .,
                      data = ROS_train_data,
                      family = binomial)
summary(ROS_model)
```

### Making Predictions on `ROS_test_data`
```{r}
ROS_test_probs <- predict(ROS_model,
                          newdata = ROS_test_data,
                          type = "response")
ROS_test_pred <- ifelse(ROS_test_probs > 0.5, 1, 0)

mean(ROS_test_pred == ROS_test_data$diabetes)
```
So from a very comfortable A, accuracy has dropped to an almost borderline B+. Have the other performance metrics improved?

### ROS Model Confusion Matrix
```{r}
ROS_confmatrix <- caret::confusionMatrix(
  as.factor(ROS_test_data$diabetes),
  as.factor(ROS_test_pred)
)

ROS_confmatrix
```
As expected, there is an increase in the value of NPV. Now that the training data has been resampled appropriately to make diabetic and non-diabetic instances as frequent, the model's increased exposure to diabetic data, despite *just being duplicates*, affects the learning process of the model. **Is this the best way to train the model though?**

### Train & Test Set Creation for RUS
```{r}
for_RUS <- copy(raw_df)

set.seed("123")

split_RUS <- caTools::sample.split(for_RUS$diabetes, SplitRatio = 0.8)

RUS_train_data <- subset(for_RUS, split_RUS == TRUE)
RUS_test_data <- subset(for_RUS, split_RUS == FALSE)

RUS_train_data
```

### RUS Resampling

RUS works in the *opposite direction* of ROS; whereas previously the minority class was duplicated to match the count of the majority class, here random rows from the majority class is removed to match the count of the minority class. The result is a smaller dataset with fewer total samples.
```{r}
RUS_targcounts <- table(RUS_train_data$diabetes)

barplot(
  RUS_targcounts,
  names.arg = c("Non-diabetic", "Diabetic"),
  col = "steelblue",
  main = "Target Variable Distribution Before RUS",
  xlab = "Classification",
  ylab = "Observation Frequency")
```

The majority class outnumbers the minority class by about the same proportions again.
```{r}
RUS_output <- ROSE::ovun.sample(
  diabetes ~ .,
  data = RUS_train_data,
  method = "under",
  N = 2*min(table(RUS_train_data$diabetes))
)

RUS_train_data <- RUS_output$data
head(RUS_train_data)
```

How does the distribution change after resampling?
```{r}
RUS_targcounts2 <- table(RUS_train_data$diabetes)

barplot(
  RUS_targcounts2,
  names.arg = c("Non-diabetic", "Diabetic"),
  col = "steelblue",
  main = "Target Variable Distribution After RUS",
  xlab = "Classification",
  ylab = "Observation Frequency")
```

Now that the training set has been balanced, it's time to train the model.

### Model Training on RUS Data
```{r}
RUS_model <- glm(diabetes ~ .,
                  data = RUS_train_data,
                  family = binomial)
summary(RUS_model)
```

### Making Predictions on `RUS_test_data`
```{r}
RUS_test_probs <- predict(RUS_model,
                          newdata = RUS_test_data,
                          type = "response")
RUS_test_pred <- ifelse(RUS_test_probs > 0.5, 1, 0)

mean(RUS_test_pred == RUS_test_data$diabetes)
```

Accuracy is about the same as the ROS model. Are there any improvements for the other model performance metrics?

### RUS Model Confusion Matrix
```{r}
RUS_confmatrix <- caret::confusionMatrix(
  as.factor(RUS_test_data$diabetes),
  as.factor(RUS_test_pred)
)

RUS_confmatrix
```

NPV and PPV seem to be about the same as their ROS counterparts.

# Results & Discussion
```{r}
methods <- c("None", "ROS", "RUS")
accuracies <- c(94.9, 88.1, 88.2)
ppvs <- c(98.7, 88.2, 88.4)
npvs <- c(64.5, 87.5, 87.3)

par(mfrow = c(1, 3))

barplot(accuracies, names.arg = methods, col = "skyblue", ylim = c(0, 100),
        main = "Accuracy", ylab = "Percentage")

barplot(ppvs, names.arg = methods, col = "lightgreen", ylim = c(0, 100),
        main = "PPV", ylab = "Percentage")

barplot(npvs, names.arg = methods, col = "salmon", ylim = c(0, 100),
        main = "NPV", ylab = "Percentage")
```

In this project a binary classification task was performed on a public health dataset containing a total of 9 features. A binary target `diabetes` was chosen to be the target of the binary classification task. Predictors consisted of 4 categorical and 4 continuous variables, all of which through EDA, were shown to be statistically significant with regards to the target variable (continuous attributes were not colinear, categorical attributes all yielded small p-values). The main challenge in performing this task was in accounting for the class imbalance of the target in the dataset, for every instance of the minority class there were about 8 of the majority class. The majority class consisted of non-diabetic observations.

Modeling was done through logistic regression, this was performed three times to see the effect of different resampling approaches: (i) no resampling, (ii) random oversampling, and (iii) random undersampling. The first iteration of modeling yielded the highest accuracy metric of 94.9%, for each prediction the model made on its test set the likelihood of it being correct was 94.9%. A deeper analysis of its performance however showed that while PPV is high, NPV was only 64.5%-the model fails to predict cases where an observation should be diabetic. Given the distribution of the target, if the model predicted non-diabetic for all observations, the accuracy would still be in the 90s.

The next two iterations of modeling applied either RUS or ROS to their training data to remove the class imbalance of the target. In ROS, random minority observations were duplicated until target frequencies balanced out. RUS on the other hand randomly removed instances of the majority class until the target frequencies balanced out. Both models performed similarly on their test sets, a loss of about 7% to accuracy but an increase to 88% in NPV. While these models trained on resampled data only get accurate predictions approximately 88% of the time, they're able to predict the diabetic case 88% of the time also-a stark increase from the non-resampled's NPV of 64.5%. Whether or not this is the case that defines the metric for model sensitivity requires input from domain experts.

This project was primarily inspired by the study done by [Deina et. al. (2024)](https://bmchealthservres.biomedcentral.com/articles/10.1186/s12913-023-10418-6). It's a paper that proposed a ML framework for data preparation and model training alongside introducing a novel resampling technique, Instance Hardness Threshold (IHT), and model,  Symbolic Regression (SR), to predict medical appointment no-shows. The paper compared performances of KNN, SVM, and SR models alongside different sampling techniques such as RUS, SMOTE, NearMiss - 1, and IHT. Through iterations of cross-validation they found that the combination of SR & IHT outperformed almost every other combination for most performance metrics.

This project's methodology initially was to compare a suite of oversampling and undersampling techniques to the performance of a KNN model and a logistic regression model. However due to limitations in compute power, made apparent in an attempt to do SMOTE, the project was downsized to just a comparison of no-resampling, ROS, and RUS when paired with logistic regression. Despite this, the project still demonstrates the benefits of resampling especially when dealing with highly imbalanced datasets. Further iterations of this project may look into improving the EDA/feature engineering/feature extraction along with using more models and resampling techniques.

# References

Excluding the dataset, the resources that were consulted in the making of this project are:

https://bmchealthservres.biomedcentral.com/articles/10.1186/s12913-023-10418-6

https://www.geeksforgeeks.org/confusion-matrix-in-r/

https://rpubs.com/SameerMathur/LR_GLM_CCDefault

https://www.youtube.com/watch?v=C4N3_XJJ-jU

https://pmc.ncbi.nlm.nih.gov/articles/PMC8529476/

https://www.cdc.gov/mmwr/volumes/72/wr/mm7210a7.htm

https://www.geeksforgeeks.org/logistic-regression-and-the-feature-scaling-ensemble/
