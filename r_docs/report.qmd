---
title: "Supervised Classification of Diabetes Data"
author: Jean Maximus C. Cacacho
format: html
---

# Introduction

The data used to perform this *report(?)* was obtained from an open-access Kaggle data repository. Accessing this data can be done through the link below:

https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset

The primary objective of this study is to perform a **binary classification task with logistic regression**, identifying diabetic from non-diabetic observations. Second to this would be examining the effect of two different resampling approaches: **(i) undersampling** and **(ii) oversampling**, to the performance of the model.

# Data Pre-processing

To yield a performant model, it's imperative for our input data to be clean. This involves pruning observations from the original data file, scaling values, and the appropriate value conversions, among other appropriate steps that may arise.

On a high level, what is done in this subsection of the *notebook(?)* are the following:

1. Loading the dataset into R studio.
2. Summary statistics informing data cleanup process.
3. Data cleanup.

## Loading Data
In this step the `diabetes_prediction_dataset.csv` file is stored into a dataframe.
```{r}
library(data.table)
raw_df <- fread("../data/diabetes_prediction_dataset.csv")
```
After this, the head and tail of the loaded dataframe was inspected to see if the file was read into the environment.

### Head
```{r}
head(raw_df)
```
### Tail
```{r}
tail(raw_df)
```

## Removing Nulls
Before further pre-processing steps (scaling, value conversion etc.), observations that aren't *complete* will be removed. Essentially rows with missing data are to be discarded from the considered dataset.

### Counting Nulls / Attribute
```{r}
colnames(raw_df)
```
From this cell it can be gathered that there are 9 attributes all in all. From the whole dataset with respect to each of these attributes, we count the amount of nulls.
```{r}
null_counts <- raw_df[, lapply(.SD, function(x) sum(is.na(x) | trimws(x) == ""))]
print(null_counts)
```
The result of the previous cell shows that all rows have complete data.

## Value Conversions
Part of data pre-processing is also ensuring that observations are stored in programmatically convenient formats. In the context of R an example of this would be storing categorical/discrete data as `factor`s.

`str(raw_df)` can be used to inspect how these values are stored in the dataframe.
```{r}
str(raw_df)
```
The previous query showed that the following attributes: (i) gender, (ii) hypertension, (iii) heart_disease, (iv) smoking_history, and (v) diabetes, are categorical. However, the attribute smoking_history seems to have more than just 2 possible values.

To be sure, an inspection of all unique values in these attributes ascertains whether or not the value is binary or not.
```{r}
categorical_attr <- c("gender", "hypertension", "heart_disease", "smoking_history", "diabetes")

unique_vals <- lapply(raw_df[, ..categorical_attr], unique)
print(unique_vals)
```
It can be seen that the attributes: hypertension, heart_disease, and diabetes, already only have two possible values; gender and smoking_history however do not.

### Handling `gender` Attribute
*Surface level research* shows that individuals with non-binary gender identities show higher incidence rates for other diabetes related comorbidities, e.g. smoking [(Tan et. al. 2021)](https://pmc.ncbi.nlm.nih.gov/articles/PMC8529476/#:~:text=Transgender%20and%20gender%2Dexpansive%20(TGE)%20adults%E2%80%94individuals%20who%20have,to%20smoke%20cigarettes%20than%20cisgender%20individuals%20[3].). This suggests that gender identities beyond just male and female may provide the model with sufficient information, along with other features, for classification.

| gender | description                          | encoding |
|------------------|--------------------------------------|----------:|
| Male            | Individual whose sex is Male                  |         0 |
| Female           | Individual whose sex is Female                  |         1 |
| Other      | Individual with non-binary gender identity       |         2 |

These encodings serve only to be labels that are programmatically convenient to work with in processing.
```{r}
gender_encodings = c(
  "Male" = 0,
  "Female" = 1,
  "Other" = 2
)

raw_df[, gender_code := gender_encodings[gender]]
print(raw_df)
```
It can be seen that `gender` has been successfully encoded into `gender_code`. As such, the original column can now be discarded.
```{r}
raw_df[, gender := NULL]
print(raw_df)
```
### Handling `smoking_history` Attribute
The earlier cell that was supposed to detect and count for null values was not able to catch `smoking_history` observations marked with `No Info`. That was because the previous function looked for values stored in R as `NA` or as empty strings. Including these observations may just be *noise* for the classifier, as such observations with `No Info` in `smoking_history` are discarded.
```{r}
row_count_before <- raw_df[, .N] # row count before drop
print(paste("Row count before drop: ", row_count_before))

raw_df <- raw_df[!(smoking_history == "No Info")]

unique(raw_df$smoking_history)
row_count_after <- raw_df[, .N] # row count after drop
print(paste("Row count after drop: ", row_count_after))
```
To turn the remaining string values into meaningful numerical data, **ordinal encoding comes to mind.** Notice how the magnitude of `smoking_history` can be surmised from the current values; those who have never smoked have, by definition, smoked less than those who were former smokers. Through the same intuition, it can be said that current smokers smoke the most. Formal definitions for these terms can be obtained from [online medical sources](https://www.cdc.gov/mmwr/volumes/72/wr/mm7210a7.htm).

From this information these encodings were generated:

| smoking_history | description                          | encoding |
|------------------|--------------------------------------|----------:|
| never            | never smoked before, or has smoked less than 100 cigarettes in their lifetime                  |         0 |
| former           | smoked at least 100 cigarettes in their lifetime but does not currently smoke                  |         1 |
| not current      | smoked at least 100 cigarettes in their lifetime but does not currently smoke       |         1 |
| ever             | term encompassing both current and former smokers, anyone who has smoked at least 100 cigarettes in their lifetime     |         1 |
| current          | smoked at least 100 cigarettes in their lifetime and currently smokes                        |         2 |

It must be noted that in the context of this project, the terms `former`, `ever`, and `not current`, were not defined in the data card. As such, operative definitions were gleaned from the CDC *QuickStats* article hyperlinked above.
```{r}
smoking_encodings <- c(
  "never" = 0,
  "former" = 1,
  "not current" = 1,
  "ever" = 1,
  "current" = 2
)

raw_df[, smoking_code := smoking_encodings[smoking_history]]

print(raw_df)
```
It can be seen that `smoking_history` has been encoded successfully into the column smoking_code; the old column can now be safely discarded.
```{r}
raw_df[, smoking_history := NULL]

print(raw_df)
```
Now that the data is clean, categorical columns can now be converted to the `factor` data type.
```{r}
categorical_attr2 <- c("gender_code", "hypertension", "heart_disease", "smoking_code", "diabetes")

raw_df[, (categorical_attr2) := lapply(.SD, as.factor), .SDcols = categorical_attr2]

# verify changes
str(raw_df)
```

# Exploratory Data Analysis

The goal in exploratory data analysis is discovering correlations and distributions in both dependent and independent variables. This is important as in this project, it serves as the main guide for feature selection.

Excluding the target, the current cleaned table has 8 attributes of which 4 are categorical (`hypertension`, `heart_disease`, `gender_code`, and `smoking_code`) and the other 4 (`age`, `bmi`, `HbA1c_level`, `blood_glucose_level`) are continuous.

## Analyzing Target Distribution

One of the biggest hurdles in classification tasks is class imbalance in the data; when there are more observations for a certain dependent variable than others then there is a class imbalance. In the context of the project the dependent variable is the `diabetes` attribute.

```{r}
library(ggplot2)

class_imbalance_data <- raw_df[, .(diabetes)]
counts <- class_imbalance_data[, .N, by=diabetes]

ggplot(counts, aes(x = factor(diabetes), y = N)) + 
  geom_bar(stat="identity", fill="steelblue") + 
  labs(
    title = "Target Variable Distribution",
    x = "Diabetes (0 = Non-diabetic, 1 = Diabetic)",
    y = "Number of Observations"
  ) + 
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))

```

Visually it can be seen that the data is **greatly imbalanced**.

## Examination of Continuous Attributes

For this subsection we consider the continuous attributes of the dataset and how they relate to the target variable.
```{r}
continuous_attr <- c("age", "bmi", "HbA1c_level", "blood_glucose_level", "diabetes")

continuous_eda <- raw_df[, ..continuous_attr]
print(continuous_eda)
```

## Examination of Categorical Attributes

For this subsection we consider the categorical attributes of the dataset and how they relate to the target variable.
```{r}
categorical_attr3 <- c("hypertension", "heart_disease", "gender_code", "smoking_code", "diabetes")
categorical_eda <- raw_df[, ..categorical_attr3]

print(categorical_eda)
```
## Examination of All Data Attributes

For this subsection we examine the interaction of the categorical and continuous data.
```{r}
print(raw_df)
```

# Modeling

## Scaling Continuous Attributes

https://www.geeksforgeeks.org/logistic-regression-and-the-feature-scaling-ensemble/

An inspection of the data's continuous attributes (e.g. `age`, `bmi`, `blood_glucose_level`) makes apparent the difference in *scale* that these values have. From a numerical standpoint, measurements in `bmi` and `age` are significantly less than measurements in `blood_glucose_level`.

Feature scaling ensures that all features contribute equally to the model's learning process. Numerically large features, when not scaled properly, may dominate learning over smaller-scale features; `blood_glucose_level` may, by virtue of the units it's measured in, overtake `HbA1c_level` in contribution because of the difference in their values.